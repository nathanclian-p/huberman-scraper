The methods were performed in accordance with relevant guidelines and regulations and approved by the Advarra review board.
Participants
Individuals over the age of 18 years were eligible to participate. Those with a prior diagnosis of diabetes who were not taking insulin were eligible, as were those with prediabetes or no history of glucose abnormalities. There were no body weight or BMI restrictions. Only participants who signed a disclaimer to use deidentified data (DID) were included. An external review board (Advarra) confirmed that analysis of DID was exempt from requiring formal informed consent.
The season of Me program for personalized metabolic health management
The Season of Me (SoM) program was designed to leverage technologic advances to improve glucose time in range and weight loss in individuals with or at risk for T2D. Participants paid to use a mobile-app (January AI) and wear a CGM (Freestyle Libre, Abbott) and HR monitor (Apple Watch or Fitbit) for 28 days (Fig.
1
). The mobile app integrated CGM and HR data with user-entered diet and activity data, along with wearable-tracked HR. In addition to providing integrated data back to users, the program provided individualized recommendations based on data both logged by users and pulled from users’ wearable devices.
The first 14 of 28 days consisted of an experimental phase, during which participants were monitored using a CGM and a HR monitor. The initial 4 days of this period served as a baseline, during which participants continued their regular diet and activities. On Day 3, participants undertook a glucose shot test. Commencing from Day 5, time-restricted feeding was introduced based on a schedule suggested by the app, and specific food experiments were initiated (e.g., ‘Low Glycemic Load first meal’ on Day 6).
Upon completion of the 14-day experimental phase, participants received a personalized report. This report compiled health insights derived from the data collected during the experimental phase, and provided tailored recommendations aimed at improving glycemic control, focusing on the enhancement of TIR.
Subsequent to the experimental phase, participants transitioned into a 15-day Time in Range Improvement Phase. During this period, daily tasks, structured as task cards within the mobile application, were assigned to participants. Each day involved the completion of 3–5 specific tasks, including mandatory reading of educational content (Fig.
8
).
Fig. 8: Example task card.
Such a task card would have been presented to participants in the Season of Me program via their mobile application. Users can indicate whether or not the task was performed by toggling the button indicated.
For Days 1–14, tasks predominantly consist of adherence assignments (for example, “Wear your HR monitor for at least 23 h today”); food experimentation tasks; and time-restricted eating tasks. From Days 15–30, tasks were primarily centered on interventions to enhance TIR. These encompass strategies such as carbohydrate reduction, caloric restriction, exercise regimens, time-restricted eating protocols, and mindfulness practices.
To facilitate participant learning throughout the 30-day journey, Insight Cards were provided via the app. These cards were categorized into Program-related Insight Cards and General Insight Cards. Program-related Insight Cards provided comparative analyses between different days and conditions (e.g., “Day 3 vs Day 4 vs Day 6”) to visualize the impacts of various interventions on blood glucose levels. General Insight Cards include Food Cards and Activity Cards, which illustrated the glucose and HR responses to a specific food item or physical activity, respectively.
Following the initial 28 day phase, a 2-month optional second phase followed, during which participants implemented the learnings without the use of CGM or HR monitor, relying only on personalized recommendations generated by the mobile app. These recommendations centered around the following five levers (Fig.
2
):
1. Reducing spiking foods. Based on users’ food logs and CGM data, the app identified the foods that caused the largest increase in blood glucose, and offered alternative, lower-glycemic foods.
2. Calorie restriction. The app gave users personalized total caloric recommendations based on age, weight, height, sex, and physical activity at baseline, with recommendations to stay below the caloric requirement for weight maintenance.
3. Increasing fiber intake. After an observational period during which users’ baseline fiber intake was observed, the app suggested fiber-rich foods to increase daily fiber consumption to 21–25 and 30–35 g/day for females and males, respectively. The app not only identified users’ existing sources of fiber, but also suggested alternative foods with higher fiber content.
4. Increasing activity. After an observational period during which users’ baseline physical activity level was observed, the app suggested activity, especially post-meal activity, with the goal of reducing postprandial glucose spikes.
5. Increasing fasting period. The app recommended a 16-h fasting target to all users, tracking users’ fasting periods and comparing observed fasting activity against goal.
Delivery of personalized food and activity recommendations
Health goals were addressed by a proprietary mobile application that incorporates CGM and HR monitoring coupled with food and activity tracking, and generates glucose predictions for food and activity patterns.
We utilized two blood glucose prediction models (“Continuous Glucose Prediction model”/“CGP model” and “Food Recommendation model”/“FR model”). The former utilizes a machine learning-based algorithm that takes into account the user’s previously recorded blood glucose, heart rate, and food logging information to output the user’s predicted blood glucose values in response to food and activity. The latter recommends to the user foods similar to those the user desires to eat, in order to allow the user to choose foods which cause comparatively lower spikes in glucose. Those models are described in detail below.
Continuous Glucose Prediction (CGP) algorithm
Overview
The machine learning-based algorithm has a base of data that is collected from all users, comprising 46,655 days of data from 1978 users; based on an individual’s entered and captured data, the model is fit to their unique glycemic responses to food and exercise as captured by CGM, HR, and food logging data.
The CGP model has two primary utilities. First, this model allows users to predict the glycemic impact of food 2 h into the future, and without consuming the food item(s) in question (“
CGP
”). Second, this model allows users to continuously estimate blood glucose values throughout the day, if provided information about food logging and heart rate alone (“Virtual CGM” or “
VCGM
”). This aspect of the algorithm allows for continued, personalized recommendations in-between CGM usage periods, thus lessening the user burden and cost associated with physical CGM devices.
The CGP model requires a minimum of 5 days of complete data (12 h of HR and CGM data, in addition to logs of all calorie-containing food and beverage, constitute a day of “complete” data), but continues to fit itself to the individual if the individual continues wearing a CGM and heart rate monitor, and logging food.
The CGP model was an RNN model. The RNN model consisted of a single LSTM layer followed by a dense layer with a sigmoid activation function. The LSTM layer allows the model to capture long-term dependencies in the input sequence. We tried both LSTM and GRU layers with different numbers of nodes and layers. Our evaluation of our model showed that, based on our data, LSTM was the best choice, giving us better RMSE and MAPE.
We furthermore used a meta learning algorithm to optimize the hyperparameters of the RNN model. The meta learning algorithm uses a set of training tasks to learn the optimal hyperparameters for the RNN model. The training tasks consisted of subsets of the dataset. For each training user, the meta learning algorithm generated a set of hyperparameters that optimized the RNN model’s performance on the specific user.
The error in these predictions is 13.4 mg/dl RMSE over the 2 h post-meal period. This error is on par with the best-published results, which do not report this high variance, post-meal period
46
Notably, the 2-h, post-meal, glucose prediction error remained low at 14.8 mg/dl RMSE, even after participants stopped using their glucose monitors, suggesting that the app was able to learn an individual’s biology sufficiently well so as to predict their glucose response.
Data and preprocessing
Our dataset consisted of participant-derived continuous glucose measurements, heart rate readings, physical activity, time stamps, and dietary constituents. This resulted in a time-series database for each variable, offering a rich, multi-modal representation of individual physiological profiles.
Initial preprocessing was conducted to assure the suitability of data for machine learning algorithms. This process comprised the removal of aberrant or incomplete data entries and the standardization of all input features to maintain consistency across the dataset. Additional features were engineered from the raw data to enhance the predictive power of the model. For example, time of day was represented as sin and cos functions to ensure temporal continuity. The preprocessed dataset was bifurcated into a training subset for model learning; and a testing subset for subsequent model performance evaluation.
Model architecture
The model architecture consists of Long Short-Term Memory (LSTM) layers and Dense layers. The LSTM layers are designed to capture temporal dependencies in the time series data, while the Dense layers provide non-linear transformations and help in the final prediction. The input to the model at each time step includes the following features: continuous glucose values, heart rate, exercise, time of day, and food nutrients. These features are concatenated and fed as input to the model. To train the model, sequences of input data are created from the training set. Each sequence contains a fixed number of consecutive time steps and associated target values (e.g., the next glucose value). The sequences are created by sliding a window over the time series data. The model is trained using the generated sequences from the training set. During training, the model predicts the next step given the current input, and the previous prediction is fed back as an input for the next time step. This feedback loop helps the model learn from its own predictions. The model is trained by minimizing the loss function, log likelihood loss function was used, between the predicted values and the true target values. The ADAM optimizer was used to perform backpropagation and gradient descent algorithms. Hyperparameters, such as the number of LSTM and Dense layers, the size of each layer, learning rate, and batch size, were tuned to optimize the model’s performance. After training, the performance of the model was evaluated using the testing set. Various evaluation metrics, such as root mean squared error, correlation coefficient, and MAPE were computed to assess the accuracy and reliability of the predictions. Based on the evaluation results, further refinements may be made to the model. This could involve adjusting hyperparameters, modifying the architecture, or adding regularization techniques to improve generalization and prevent overfitting. Once the model is trained and evaluated, it can be used to make predictions on new, unseen data. Given a sequence of input features, the model can generate predictions for the next time step(s) of blood glucose values (Fig.
9
).
Fig. 9: Overview of the Machine Learning Pipeline for Prediction of Blood Glucose Values.
This figure illustrates the machine learning pipeline designed for predicting future blood glucose levels based on various inputs, including continuous blood glucose measurements, food nutrients, heart rate, exercise, and time of day. The pipeline consists of several steps, starting with data preprocessing, followed by the utilization of a recurrent neural network (RNN) comprising LSTM (Long Short-Term Memory) and Dense layers.
A
The process begins by collecting and preparing the input data, which encompasses continuous blood glucose readings, food nutrient information, heart rate data, exercise data, and time of day. The collected data then undergoes preprocessing, where it is cleaned, normalized, and organized in a suitable format for the subsequent stages.
B
Next, the preprocessed data is fed into the RNN model, which is composed of LSTM and Dense layers. The LSTM layers are employed to capture temporal dependencies and patterns within the data, enabling the model to understand the sequential nature of blood glucose fluctuations over time. The Dense layers aid in learning complex relationships and extracting relevant features from the input data.
C
The RNN model is trained to predict the blood glucose level for the next time step. Once the initial prediction is made, it is fed back into the model as an input, allowing the model to generate subsequent predictions for future time steps. This feedback loop enables the model to iteratively refine its predictions and adapt to changing conditions.
D
The output of the pipeline is a sequence of predicted blood glucose values, which can be used for various applications, such as monitoring and managing blood glucose levels in individuals with diabetes or supporting personalized dietary and exercise recommendations. Overall, this machine learning pipeline offers a systematic approach for blood glucose prediction, leveraging data preprocessing and a recurrent neural network architecture with LSTM and Dense layers to provide accurate and timely forecasts of blood glucose levels.
We also wished to determine the most significant inputs (among CGM, heart rate, etc.) to the performance of the CGP model. Ablation analysis was conducted (Fig.
10A
), removing inputs to the model in sequence to examine the deleterious effects on the model. The difference in the RMSE as a result of ablating nutrient information versus activity information demonstrates that the macronutrients and their quantity consumed are far more important than activity, heart rate, and time of day to the model’s fidelity.
Fig. 10: Reliance.
A
Use of ablation analysis to determine feature importance. Starting from the left, the most important modality is removed and the model is retrained to measure the impact of the removed dataset. This analysis demonstrates in order of descending importance the significance of each input was as follows: nutrients, time of day, heart rate, activity.
B
Similar to (
A
) but in each iteration the least important dataset is removed.
This was followed by reverse ablation analysis (Fig.
10B
). While ablation analysis removes inputs to determine which input is most important, reverse ablation analysis adds inputs. The reverse ablation analysis shows that, by adding activity, HR, and time of day information, our RMSE increases from 12.5 to 14. However, upon adding nutrient information, RMSE increases to 21. This demonstrates the dramatic effect that nutrition information has on the fidelity of the model.
Overall, both analyses led us to conclude that, in order of descending importance, the significance of each input was as follows: nutrients, time of day, heart rate, activity.
Evaluation of CGP model
Data Collected
DID was collected retrospectively from 2217 users over 28 days. Data points included body weight, CGM data, food and activity log data, and HR from an activity tracker. Nutritional breakdown included total caloric intake, macronutrient composition, and fiber intake, which was captured by participants’ self-reported food logs collected in the mobile application. Activity was quantified in minutes/day. Only participants with complete logging and data capture were included in the final analysis (see below). Among those included in the final analysis, adjustments were made to account for potential bias in nutrient intake as a result of differences in logging frequency over time: (1) nutrients were calculated only from “good” logging days, defined as a minimum of two logging events spread throughout waking hours spanning a 16 h range, and a total of ≥1600 calories logged, and (2) specific macronutrients and fiber were presented as grams as well as proportion of total calories (grams converted to calories) such that the changes in the proportion of these nutrients were not biased by residual differences in frequency of logging. Physical activity was adjusted for overall frequency of logging and expressed as adjusted minutes/day. HR data was collected continuously, as was CGM data, both from wearable devices, and did not rely on participant adherence with logging; thus, this data is not subject to logging bias. For CGM, the first day of use is known to be somewhat less accurate, and thus all glycemic measures excluded the first day of use in all participants. Further adjustments were made for days with loss of CGM signal, and this proportion of “lost time” was applied to all measures of event frequency. The average measures such as TIR and GMI did not require this adjustment. HR data capture was remarkably consistent and no gaps in signal were present.
Requirements for Inclusion in data analysis
In order to ensure that only individuals who had complete data capture and reasonably consistent data logging were included in the final analysis, the following requirements were designated. Analysis of outcomes included only those individuals who had a sufficient quantity of CGM data capture, consistent food logging, and regular body weight tracking and HR capture. Requirements for CGM data were at least 70% CGM coverage on at least half of the days at the beginning (days 1–5, excluding day 1) and the end (days 15–27) of the 28 day period. Requirements for meal logging were active logging of all meals during the first 7 days, as well as the last 14 days, and HR capture ≥20 h per day. For inclusion in data analysis, users must have logged at least two meals and 1600 kcal/day. Requirements for body weight data tracking were at least one body weight measurement in the first 7 days and in the last 14 days. Because fewer individuals tracked weight at day 28, the analysis of weight change was conducted only in the subset who had the baseline and end of study weight measurements.
Statistical analysis
Data was measured using paired-student
t
-tests for beginning vs. end of study for all measures, using Jupyter Notebooks, SciPy, Numpy, Pandas, Matplotlib, Seaborn, pickle, and datetime (the latter two are inbuilt Python packages). Beginning of study was defined as days 2–7 for glucose variables; and days 1–5 for activity and food logging variables. All variables were checked for normality and none needed log transformation for analyses. End of study was defined as days 14–28 for all measures. P < 0.05 was considered statistically significant. The performance of the CGP model was shown to be superior when compared to other models (Fig.
11A
), with a high correlation coefficient of 0.833 when comparing actual BGL to predicted BGL (Fig.
11B
). A visual depiction of CGP compared to actual curves shows that the actual curve lies within the error bound of the CGP model. Figure
11C
shows a comparison of glucose prediction to its corresponding actual CGM curve within a 2-h window, while Fig.
11D
shows a comparison of virtual CGM prediction to its corresponding actual CGM curve within a 24-h window. We also examined whether there was a correlation between user demographics and error, in order to determine whether users of certain demographics were more prone to higher error in BGL predictions, and found that higher weight correlates with higher percent error (p < 0.05), and higher age correlates with lower percent error (p < 0.05) (Fig.
11E
). Furthermore, we compared error by disease type and gender, in order to determine whether male/female participants, or normoglycemic/prediabetes/T2D participants were more likely to experience higher error in BGL predictions. Our percent error and RMSE peak/point-by-point was lower for healthy users than for users with T2D and with pre-diabetes. Our percent error was also lower for male participants than for female participants, though there are participants whose percent error represents an outlier (Fig.
11F
).
Fig. 11: Understanding the performance of the CGP model.
A
Evaluation of CGP model performance against that of comparable models (prediction of the glucose impact of certain foods, “CGP”; and prediction of a glucose curve when given consistent food logging and heart rate information, “VCGM”) to that of several other models. Across a number of dimensions, including RMSE Peak, RMSE point by point, RMSE point by point shifted, correlation, and percent error, we found that the CGP model outperformed each of its competitors.
B
Comparison of actual blood glucose values versus blood glucose values predicted by the CGP algorithm. The correlation coefficient is 0.83, and holds more strongly for non-outlier values (<200 mg/dL). The reasons for this are twofold: first, because BGL fluctuation for these users is generally high; second, because extreme outlier BGL values are rare, and thus appear far less frequently in our training set.
C
Comparison of CGP predicted blood glucose values versus actual blood glucose values. The CGP algorithm operates on a stochastic basis, generating at each 15-min time interval 100 different potential BGL values, along with the corresponding likelihood of each value occurring. The red line reflects CGP predicted BGL; the green line represents BGL values derived from CGM. The orange zone represents the range of BGL values between the 25th and 75th percentiles of the CGP predictions, by likelihood of occurrence; the blue zone represents the 10th and 90th percentiles. As shown, the CGM-derived curve falls within the confidence interval of the predicted curve.
D
Comparison of VCGM predicted blood glucose values versus actual blood glucose values over a 24-h period. The red line reflects CGP predicted BGL; the green line represents BGL values derived from CGM. The orange zone represents the range of BGL values between the 25th and 75th percentiles of the CGP predictions, by likelihood of occurrence; the blue zone represents the 10th and 90th percentiles. As shown, the CGM-derived curve falls within the confidence interval of the predicted curve.
E
Correlation between demographics and errors. We examined whether certain demographic information correlated with higher instances of error. We found that higher weight correlates with higher percent error (
p
< 0.05), and higher age correlates with lower percent error (
p
< 0.05).
F
Comparisons of error by gender and disease type. We found that percent error and RMSE peak/point-by-point were lower for healthy participants than for participants with prediabetes/T2D; and lower for males than for females.
Food recommendation algorithm
Food recommendations utilize a separate food recommendation engine underpinned by the CGP model, which predicts the glycemic impact of selected foods based on their macronutrient composition and the individual’s prior responses to macronutrients. The food recommendation engine extrapolates nutritional information and macronutrients from known databases to recommend similar foods that are predicted to be less impactful on a user’s blood glucose, based on the individualized output of the CGP model. The food recommender engine follows four main processing stages, with the user input being a specific food, and the result being similar foods with lower glycemic impacts. The food recommender engine winnows down the food database to find more similar foods, then healthier foods within the set of similar foods (Fig.
12A
). Similar, healthier foods are then displayed to the user in-app as “healthy recommendations” (Fig.
12C
). The model interfaces with the application via a “FoodRec Client” interface (Fig.
12B
).
Fig. 12: Overview of the Food Recommender model.
A
Food Recommender processing stages. Conceptually, the Food Recommender can be organized as a sequence of 4 processing stages: Input food →(1) Personalized similarity food matching. Finding food items similar to the food item being searched by the user →(2) Blacklist foods removal. Removal of foods that the user has “blacklisted”, i.e. allergies, sensitivities →(3) Healthiness ranking. Ranking of foods according to a “healthiness” scale that takes into account carbohydrate and fiber composition →(4) Cleanup of recommendations list. Removal of substandard recommendations→ Recommendations for user.
B
Interface between app and Food Recommender.
C
Example in-app recommendations. Note the similarity between the food item being looked up, and the food item being recommended to the user.
Reporting summary
Further information on research design is available in the
Nature Research Reporting Summary
linked to this article.